{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Data\n",
    "\n",
    "**Web Scraping** (also called screen scraping, web data extraction, web harvesting, etc.) is a technique to extract data from a website which can then be saved to a file, database, or used in an application.\n",
    "\n",
    "Data displayed by some websites are only viewable through a web browser and may not have the functionality to download a copy to a file. Rather than manually copying and pasting the data - a time consuming and tedious task - web scraping automates this process. This makes the task of collecting the data more efficient and the program can be scheduled to web scrape new data that is added in the future.\n",
    "\n",
    "In this lesson, we will use the `requests` and `bs4`(Beautiful Soup) libraries to extract the data from a Wikipedia webpage of U.S. state capitals, store the data into a `pandas` dataframe, then save the collected data to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to the website\n",
    "\n",
    "`requests` is a commonly used library to allow a Python program to access a website and its contents. The `.get()` function \"talks\" to the website and in turn, the website sends a **response** with a **status code** that states if the user can proceed within the website. For more information about status codes and their meanings go to the REST API Tutorial website's [list of HTTP status codes](https://www.restapitutorial.com/httpstatuscodes.html). Common status codes are `200 (OK)` , `401 (Unauthorized)`, `404 (Not Found)`, and `500 (Internal Server Error)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://simple.wikipedia.org/wiki/List_of_U.S._state_capitals\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the website\n",
    "# website returns a response\n",
    "\n",
    "response = requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the status code\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML Basics\n",
    "The basic syntax of an HTML webpage contains **tags** which create the structure of the website:\n",
    "\n",
    "- HTML documents must start with a type declaration `<!DOCTYPE html>` tag.\n",
    "- The HTML document is contained between `<html>` and `</html>`.\n",
    "- The meta and script declaration of the HTML document is between `<head>` and `</head>`.\n",
    "- The visible part of the HTML document is between `<body>` and `</body>` tags.\n",
    "- Title headings are defined with the `<h1>`  through `< h6>` tags.\n",
    "- Paragraphs are defined with the `<p>` tag.\n",
    "\n",
    "Other useful tags include `<a>` for hyperlinks, `<table>` for tables, `<tr>` for table rows, and `<td>` for a single cell within a table row.\n",
    "\n",
    "## Web Scraping Guidelines\n",
    "\n",
    "- Check a websites Terms & Conditions before you scrape it. Add `/robots.txt` to the end of your URL to find out if the website authorizes web scraping.\n",
    "- Carefully read the statements about legal use of data. Many times, web scraped data cannot be used for commerical purposes.\n",
    "- Do not aggressively request data from the website, as this may cause time out issues. Create your program to make infrequent or scheduled requests in order to simulate human-like behavior. Good practice suggests 1 request per webpage per second.\n",
    "- Inspect the HTML structure of the webpage through your browser before using Python to parse it.\n",
    "- The layout of a website may change over time, so revisit the site and update your code as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Parse HTML\n",
    "\n",
    "`bs4`(Beautiful Soup) is the most popular library for extracting data from HTML and XML files.\n",
    "\n",
    "The `.text` function will download a string version of the HTML structure of the requested webpage. Then using the `BeautifulSoup()` function, the text will be **parsed** (interpreted as) HTML. This will add functionality to search for tags within the webpage without treating the contents as individual string characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the first 300 characters in the string\n",
    "print(response.text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the string as HTML\n",
    "raw_HTML = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_HTML.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find HTML Elements\n",
    "\n",
    "Because there is only one `<table>` tag on this page, we can use the `.find()` function to search for the tag, which will return its contents. If there are multiple tags of the same type, the `.find()` function will return the first tag of that type listed on the page or you can add the argument `class_=` to search for a tag with a particular class type.\n",
    "\n",
    "Then within the table, we will gather all of the `<tr>` tags to collect data from each row. `.find_all()` creates a list where each item in the list is a tag of that type. This is useful for extracting data from tags that have similarly structured information, such as hyperlinks in `<a>` tags or row information in `<tr>` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for the <table> tag on the page\n",
    "raw_HTML.find('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can also search the same table using its class type\n",
    "raw_HTML.find('table', class_= 'wikitable sortable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save table to variable\n",
    "table = raw_HTML.find('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find_all creates a list of all <tr> tags in table\n",
    "# display the first 2 <tr> tags and their contents\n",
    "table.find_all('tr')[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two `<tr>` tags in the table contain `<th>` tags to create the column headers on the page. The dataset is small, so we will not focus on collecting the text from those tags to use as the column headers in the dataframe. However, because we are collecting the data from the rest of the table, we will use list slicing to skip over those rows. This will start directly at the first row that has `<td>` tags which has the actual data from the table that we want to store into the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the contents of the table that we will collect data from\n",
    "# tabledata is a list\n",
    "tabledata = table.find_all('tr')[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display first 2 <tr> tags and their contents\n",
    "tabledata[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within each `<tr>` tag, we will need to create another list (using the `.find_all()` function) of all of its `<td>` tags. In each `<td>` tag, we can access the data that is each individual table cell and extract its text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first row in the table data\n",
    "tabledata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first <td> in the first table data row\n",
    "tabledata[0].find_all('td')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the list of the first row's <td> tags\n",
    "first_row = tabledata[0].find_all('td')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loop through each <td> tag\n",
    "# print out each tag's text data\n",
    "for data in first_row:\n",
    "    print(data.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: The `Notes` column in the table does not have information for every state. In the example above, Alabama does not have any notes displayed. However, there is a hidden newline value in the print display of the text. The newline will be stored as numpy \"null\" value when we collect the data. This is important because missing/empty information needs a \"null\" placeholder when creating a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Data\n",
    "\n",
    "Now that we have identified the structure of the table, we can set up an empty dictionary to collect the values. The keys of the dictionary will later become the column headers in the dataframe. The value of each key will be a list that has information for that column in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty dictionary to hold the values\n",
    "state_info = {'State':[],\n",
    "              'Abbreviation':[],\n",
    "              'Statehood':[],\n",
    "              'Capital':[],\n",
    "              'Capital_Since':[],\n",
    "              'Area':[],\n",
    "              'City_population':[],\n",
    "              'Metro_population':[],\n",
    "              'State_rank':[],\n",
    "              'US_rank':[],\n",
    "              'Notes':[],}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_info.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, item in enumerate(state_info.values()):\n",
    "    print(index, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through each <tr> tag\n",
    "for row in tabledata:\n",
    "    \n",
    "    # get a list of <td> tags for that row\n",
    "    row_data = row.find_all('td')\n",
    "    \n",
    "    # get a list of the values (empty lists) from the dictionary\n",
    "    # loop through the index number and each item (empty list)\n",
    "    for index, item in enumerate(state_info.values()):\n",
    "        \n",
    "        # access the data from the <td> tag with the same index position as the empty list\n",
    "        data = row_data[index].text\n",
    "        \n",
    "        # check if the data is a newline or empty string\n",
    "        # if so, store a null value\n",
    "        if (data == \"\\n\") or (data == ''):\n",
    "            item.append(np.nan)\n",
    "        \n",
    "        # otherwise, store the actual data from that cell\n",
    "        else:\n",
    "            item.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify contents in the dictionary\n",
    "# list of values for state names\n",
    "state_info['State']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_info['Notes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataframe\n",
    "\n",
    "Now that the data has been extracted from the table, we can use `pandas`'s `.DataFrame()` function to structure the dictionary as a dataframe. The keys from the dictionary will be the column headers and the list for each key will become the column values. Lastly, we will save the dataframe's contents into a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe from dictionary info\n",
    "state_df = pd.DataFrame(data=state_info)\n",
    "state_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new file and save dataframe contents\n",
    "state_df.to_csv('stateinfo.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
